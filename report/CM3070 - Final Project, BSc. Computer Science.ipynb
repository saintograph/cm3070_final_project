{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "434775bc",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "{\\huge Front Matter}\n",
    "\\end{center}\n",
    "\n",
    "---\n",
    "\n",
    "\\\n",
    "\n",
    "\\hfill \\break\n",
    "\n",
    "Github repository: [https://github.com/saintograph/cm3070_final_project](https://github.com/saintograph/cm3070_final_project)\n",
    "\n",
    "\\\n",
    "\\hfill \\break\n",
    "\n",
    "\\underline {\\large Contents:}\n",
    "    \n",
    "\n",
    "1. Introduction\n",
    "2. Literature Review\n",
    "3. Design\n",
    "4. Implementation\n",
    "5. Evaluation\n",
    "6. Conclusion\n",
    "7. Appendix A: Bibliography\n",
    "8. Appendix B: User Guide\n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7de85",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "{\\huge Abstract}\n",
    "\\end{center}\n",
    "\n",
    "Intelligent Agents(IAs) are entities which are able to make decisions _and_ take actions, based on feedback received from the environment. There are several popular frameworks and paradigms in this space; this project explores two of them - Finite State Machines (FSM) and Deep Reinforcement Learning (DRL). The IAs will interact with a first-person shooter environment based on a popular video game from the 90s, Doom. Each agent will be scored based on how well it does while attempting to complete a test scenario. Getting hurt and wasting ammo would adversely affect its score, killing enemies and collecting armour would enhance it.\n",
    "\n",
    "The agent implemented with FSM is labeled 'Kane', while the agent implemented with DRL is labeled 'Abel'. Two DRL algorithms will be tested before presentation as the better performing algorithm for Abel. A discussion at the end of the report will attempt to objectively outline what went well, what did not and what can be done to improve future work.\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a3942",
   "metadata": {},
   "source": [
    "#### _Plagiarism Statement_\n",
    "\n",
    "This project was written by me and in my own words, except for quotations from published and unpublished sources which are clearly indicated and acknowledged as such. I am conscious that the incorporation of material from other works or a paraphrase of such material without acknowledgement will be treated as plagiarism, subject to the custom and usage of the subject, according to the University Regulations. The source of any picture, map or other illustration is also indicated, as is the source, published or unpublished, of any material not resulting from my own experimentation, observation or specimen-collecting.\n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deeda32",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96162361",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "{\\huge 1. Introduction: Kane and Abel - AIs that play games}\n",
    "\\end{center}\n",
    "\n",
    "---\n",
    "\n",
    "\\par\n",
    "\n",
    "\\underline {\\large 1.1 From automation to autonomy.}\n",
    "\n",
    "\\normalsize Automation preceded artificial intelligence in replacing human labour at repetitive tasks. An example of automation is automated storage and retrieval systems common across large warehouses of logistic companies.\n",
    "\n",
    "Advancements in systems such as Warehouse Management Systems can be classified into different trends. On the one hand, some advancements aim to enhance specific processes of the warehouses, improving the effectiveness and efficiency of the whole activity. On the other hand, other advances focus on improving more general processes, using Machine Learning methods such as **Reinforcement Learning** (Cestero et. al (2022)).\n",
    "\n",
    "Large robot arms retrieve packages before placing each on long conveyor belts with precise mechanical motion. A package one size too large or incorrectly oriented will cause failure in the system. The package will drop or never even reach its destination.\n",
    "\n",
    "**Automated systems are inflexible.**\n",
    "\n",
    "Automated systems make decision by calculation, searching or lookup. Each robot arm and conveyor belt was programmed to do a task, in a specific way.\n",
    "\n",
    "The automated system needs to be able to tolerate inconsistencies. This is where Autonomous A.I. is able to make an impact, in an environment streaming unstructured, dynamic data.  Unlike conventional data stream methods built upon a shallow network structure Deep Neural Networks potentially offers significant improvement in accuracy and aptitude to handle unstructured data streams (Ashfahani et. al. (2020)).\n",
    "\n",
    "\\par\n",
    "\n",
    "\\underline {\\large 1.2 Adapting automation to a changing environment.}\n",
    "\n",
    "\\normalsize Globally, companies are facing difficult situations with ever changing business environments (markets, customers, workers, equipment). Often, automated systems built around repeatable and predictable processes are unable to change programmed behaviour in response to the changing environment.\n",
    "\n",
    "The [AI Index Report](https://aiindex.stanford.edu/report/) cites that over 120,000 AI related peer-reviewed academic papers were published in 2019. A lot of the advances are heavily publicized and make it seem like the Holy Grail of A.I. The challenge is to adopt algorithms which were successful in the lab to real-life problems.\n",
    "\n",
    "\\par\n",
    "\n",
    "\\underline {\\large 1.3 Developing autonomous algorithms in a controlled lab environment.}\n",
    "\n",
    "\\normalsize Lab based experiments conducted with simulated environments are invaluable for developing autonomous A.Is, which will be referred to as Intelligent Agents (IA). \n",
    "\n",
    "Simulations in the lab do not replicate real-world situation perfectly, but there are advantages:\n",
    "\n",
    "1. low costs\n",
    "2. absence of safety issues\n",
    "3. unlimited iterations of simulations\n",
    "\n",
    "Video-game playing IAs have gained enormous publicity in recent years, with Deep Reinforcement Learning based IAs beating humans at complex games such as Dota 2 and Starcraft 2.  From board games like Go in recent engagements with agents from Deep Mind to very high state-action dimensional games like those from the real timestrategy (RTS) and first person shooter (FPS) video game genres, Deep Reinforcement Learning has established itself, presently, as a prime construct for solving highly complex problems without direct supervision of its training (Schulze et. al., 2018).\n",
    "\n",
    "Video games are perfect environment to develop and test IAs. \n",
    "\n",
    "1. video game simulations can be built from scratch,\n",
    "2. researchers can define the rules in video games, such as physics and rewards,\n",
    "\n",
    "\\par\n",
    "\n",
    "\\underline {\\large 1.4 Project Introduction.}\n",
    "\n",
    "\\normalsize In this project, the performance of two game playing agents built from dynamic programming and deep reinforcement learning techniques are compared. The environment for testing these agents is ViZDoom, an open-source research tool based on the video game classic Doom from the 90s.\n",
    "\n",
    "An earlier example of similar work is the Reinforcement Learning bot named Arnold, winner of the 2017 ViZDoom competition. Arnold was built with Deep Q-Networks and Deep Recurrent Q-Networks and evaluated across 17 maps. Arnold  divides the problem into two phases: navigation (exploring the map to collect items and find enemies) and action (fighting enemies when they are observed), and uses separate networks for each phase of the game.\n",
    "\n",
    "For the purposes of this study, **Kane** is created with the Finite State Machine paradigm, and transitions between two phases; movement (moving towards the goal) and action (shooting at detected enemies). Kane will be tested on in single scenario, Deadly Corridor.\n",
    "\n",
    "**Abel** is built with a deep reinforcement learning algorithm. The algorithm will manage moving and action in one single scenario, Deadly Corridor. It's important to note that during the implementation phase, two algorithms, Actor 2 Critic (A2C) and Proximal Policy Optimization (PPO) will be evaluated after trials and optimization.\n",
    "\n",
    "\\par\n",
    "\n",
    "\\underline {\\large 1.5 Objectives.}\n",
    "\n",
    "\\normalsize This study has one primary objective and two secondary objectives:\n",
    "\n",
    "1. **Primary Objective**: What are the strengths and weaknesses of dynamic programming and deep reinforcement learning algorithms in a controlled lab setting?\n",
    "\n",
    "2. **Secondary Objective**: Between Proximal Policy Optimization and Actor 2 Critic, which deep reinforcement learning algorithm would be easier to tweak and train?\n",
    "\n",
    "3. **Secondary Objective**: Are reward shaping, automated hyperparameter tuning, and curriculum learning good strategies for training deep reinforcement learning-based IAs?\n",
    "\n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a9712",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "{\\huge 2. Literature Review}\n",
    "\\end{center}\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\\underline {\\large Case Study 1: Design and Implementation of an Intelligent Gaming Agent with FSM}\n",
    "\n",
    "\\normalsize Study URL: [link](https://www.researchgate.net/publication/346319155_Design_and_Implementation_of_an_Intelligent_Gaming_Agent_Using_A_Algorithm_and_Finite_State_Machines)\n",
    "\n",
    "\\normalsize From the definition of Intelligent Agents in Artificial Intelligence (Russell-Norvig, 2020), we can say an intelligent agent is anything that can perceive/observing its immediate environment and take action with respect to its observation, hence we can say an intelligent gaming agent is capable of learning/observing what goes on in the gaming environment and also act on its observation (Adegun et. al., 2020).\n",
    "\n",
    "An agent has three key steps through which is cycled through while the FSM is active. The steps are commonly known as the sense-think-act cycle. In addition to these three steps, there is an optional learning or remembering step that may also take place during this loop.\n",
    "\n",
    "The game agent _must_ have access to the environment's state to generate a decision and an action _following_ the decision. The agent pursues a pre-determined goal or destination, by navigating the environment with a localization and navigation algorithm, which for this paper, was A* search.\n",
    "\n",
    "The agent navigates a 3D environment generated with the Unity3D engine. The agent encounters NPCs which attack the agent while it navigates the 3D environment towards its goal.\n",
    "\n",
    "\\underline {Critical evaluation}\n",
    "\n",
    "\\normalsize Several aspects of this research paper were unclear.\n",
    "\n",
    "1. Navigating a 3D environment with A* Search without generating a navigation mesh is computationally expensive. 3D environments have an almost _infinite_ number of points which the agent can move to, and a navigation mesh of the map is generated before A* Search is used.\n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.6]{images/waypoints_amitp.jpg}\n",
    "        \\par\n",
    "        Figure 2.1: Navigation Mesh, retrieved from https://web.archive.org/web/20110716210236/http://www.ai-blog.net/archives/000152.html, accessed in August 2022\\par\n",
    "\\end{figure}\n",
    "\n",
    "2. The paths found by A* on eight-neighbor square grids can be approximately 8% longer than the shortest paths in the continuous environment (Nash et. al., 2010)\n",
    "3. Properties of the environment were absent, such as player weapon ammo, enemy health, etc\n",
    "4. How does the agent perceive state? How does the agent 'know' when an enemy NPC is dead, or how much ammunition it has remaining?\n",
    "\n",
    "\\bigskip\n",
    "\n",
    "\\underline {\\large Case Study 2: OpenAI Five}\n",
    "\n",
    "\\normalsize Study URL: [https://arxiv.org/pdf/1912.06680.pdf](https://arxiv.org/pdf/1912.06680.pdf)\n",
    "\n",
    "In 2019, an A.I system by OpenAI, dubbed ‘Five’, defeated a team of professional players at the game Dota 2 professionals. Victory require teamwork and collaboration, making it a milestone for A.I. \n",
    "\n",
    "Link: [https://openai.com/five/](https://openai.com/five/)\n",
    "\n",
    "The agents were trained on millions of possible scenarios with each of the 110+ ‘heroes’ available to players. Training ran from June 30th, 2018 to April 22nd, 2019, which was 10 months of training on 770 PFlops/s of processing power.\n",
    "\n",
    "The agents defeated the human Dota 2 world champions in a best of three match. The agents were able to solve the following:\n",
    "\n",
    "* Long time horizons\n",
    "* Partially observed state\n",
    "* High dimensional action and observation spaces\n",
    "\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "* The complex environment of a game such as Dota 2 requires processing power beyond the reach of unaffiliated researchers\n",
    "* The agents were specialised and are unable to play another game, for example, Starcraft II.\n",
    "\n",
    "\\underline {Critical evaluation}\n",
    "\n",
    "\\normalsize The authors of this study made it clear the A.I was highly specialised; it could only perform the tasks in the environment where it's training took place. Moving the A.I to similar game with slightly different mechanics and environment, such as League of Legends, would give poor results.\n",
    "\n",
    "These limitations underline the bottomline of employing A.I. at the time of writing; writing algorithms and implementing models for generalized tasks are still _a way off_.\n",
    "\n",
    "\\bigskip\n",
    "\n",
    "\\underline {\\large Case Study 3: Ubisoft LaForge - Deep Reinforcement Learning for Navigation.}\n",
    "\n",
    "\\normalsize Study URL: [https://arxiv.org/pdf/2011.04764.pdf](https://arxiv.org/pdf/2011.04764.pdf)\n",
    "\n",
    "Ubisoft Montreal has an A.I research unit conducting experiments on A.I systems for games. This particular study highlights real-world challenges. Pros; navigation with uncommon techniques such as grappling hooks and jumping. Cons; Poor scaling.\n",
    "\n",
    "Link: https://montreal.ubisoft.com/en/deep-reinforcement-learning-for-navigation-in-aaa-video-games/\n",
    "\n",
    "The agent was able to navigate the 3D environment built with UnityML with ray-casting, which is done by bouncing ‘light rays’ at the environment and sensing the feedback. This is known as ‘local perception’.\n",
    "\n",
    "The agent navigating a 3D environment had local perception in the form of:\n",
    "\n",
    "1. 3D occupancy map\n",
    "2. 2D depth map\n",
    "3. Scalar information about its own physical attributes (e.g. weight, height)\n",
    "\n",
    "Running computations in a game engine is costly, so the agents are trained with an off-policy RL algorithm named Soft Actor-Critic(SAC). Local perception drastically improved the sampling efficiency of the Rl algorithm.\n",
    "\n",
    "\\underline {Critical evaluation}\n",
    "\n",
    "\\normalsize Off-policy algorithms such as SAC are difficult to optimize. \n",
    "\n",
    "**Off-policy algorithms** reuse data from old policies to update the current policy. On-policy reinforcement learning (RL) algorithms have high sample complexity while off-policy algorithms are difficult to tune. Merging the two holds the promise to develop efficient algorithms that generalize across diverse environments (Fakoor et. al. 2019).\n",
    "\n",
    "\\bigskip\n",
    "\n",
    "\\underline {\\large Case Study 4: Proximal Policy Optimization (PPO)}\n",
    "\n",
    "\\normalsize Study URL: [https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "\n",
    "Proximal Policy Optimization is a reinforcement learning framework developed at OpenAI for easy implementation coupled with good baseline performance. It is a policy gradient method and can be used for environments with either discrete or continuous action spaces.\n",
    "\n",
    "Proximal Policy Optimization strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small [5].\n",
    "\n",
    "\\underline {\\large How it works}\n",
    "\n",
    "\\normalsize PPO utilizes the [Actor-Critic](https://keras.io/examples/rl/actor_critic_cartpole/) method and trains a stochastic policy in an on-policy way.\n",
    "\n",
    "As an agent takes actions and moves through an environment, it learns to map the observed state of the environment to two possible outputs (Zhao, et al., 2017).\n",
    "\n",
    "* Recommended action: A probability value for each action in the action space. The part of the agent responsible for this output is called the actor.\n",
    "* Estimated rewards in the future: Sum of all rewards it expects to receive in the future. The part of the agent responsible for this output is the critic.\n",
    "\n",
    "The policy is updated via a stochastic gradient ascent optimizer, while the value function is fitted via some gradient descent algorithm. This procedure is applied for many epochs until the environment is solved.\n",
    "\n",
    "\\centering Image credit: https://keras.io/examples/rl/ppo_cartpole/\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics{images/ppo.png}\n",
    "        \\par\n",
    "        Figure 2.2: Pseudocode for the PPO algorithm. \n",
    "\\end{figure}\n",
    "\n",
    "\\raggedright \n",
    "\\normalsize PPO attains the data efficiency and reliable performance of [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) (TRPO), while using only first-order optimization. \n",
    "\n",
    "TRPO is:\n",
    "\n",
    "* complicated\n",
    "* incompatible with architectures which includes noise (dropout) or parameter sharing\n",
    "\n",
    "PPO is:\n",
    "\n",
    "* simpler to implement\n",
    "* applicable in general settings\n",
    "* a better performer overall\n",
    "\n",
    "\\underline {Critical evaluation}\n",
    "\n",
    "\\normalsize PPO strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function, while ensuring the deviation from the previous policy is relatively small (Schulman et. al., 2017).\n",
    "\n",
    "This algorithm is tried and tested, and should be amongst the algorithms evaluated for implementing the agent Abel.\n",
    "\n",
    "\\underline {\\large Case Study 5: Reward Shaping for Deep Reinforcement Learning (A2C) in VizDoom}\n",
    "\n",
    "\\normalsize Study URL: [link](http://ceur-ws.org/Vol-3094/paper_17.pdf)\n",
    "\n",
    "This study use an unmodified Actor 2 Critic algorithm to train an agent for the ViZDoom environment. Instead, the environment’s reward function were enhanced in a robust and general way, so that with minimal human supervision (which basically consists of hyperparameter tuning) this enhancement alone would allow the agent to train faster and converge to better strategies which were initially unreachable.\n",
    "\n",
    "The authors of this study attempt to predict the transition between events (player movement, kills) and combine these with the current visual buffer.\n",
    "\n",
    "\\underline {Critical evaluation}\n",
    "\n",
    "\\normalsize It is commonly held that hyperparameter tuning is crucial for _specializing_ a model for unique tasks. Poorly tuned models parameters produce suboptimal results, missing out on model convergence, minimized loss functions and model accuracy.\n",
    "\n",
    "Reward shaping alone is not viable because agents will generalize poorly between scenarios (e.g. different levels in ViZDoom) as extrinsic rewards levels differ between levels. Training the agent in a scenario with more enemies would poorly prepare it for scenarios which require the collection of a certain power-ups to win.\n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be1bd6",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "{\\huge 3. Design}\n",
    "\\end{center}\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\\underline {\\large 3.1 Intelligent Agents}\n",
    "\n",
    "\\normalsize Amongst the earliest work on visual-based reinforcement learning for autonomous navigation and actions is by Asada et. al. (1996), who trained robots to learn soccer playing skills with reinforcement learning.\n",
    "\n",
    "Based on the IA classes defined by Russell & Norvig, Kane is classified as a **simple reflex agent**, while Abel is a **intelligent agent**. An intelligent agent perceives its environment via sensors and acts rationally upon that environment with its effectors. \n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.6]{images/simple_reflex_agent.png}\n",
    "        \\par\n",
    "        Figure 3.1: Simple Reflex Agent.\\par\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.6]{images/intelligent_agent.png}\n",
    "        \\par\n",
    "        Figure 3.2: Intelligent Agent.\\par\n",
    "\\end{figure}\n",
    "\n",
    "This project will implement IA's interacting with the **VizDoom** environment, which is a first person shooter game based on the classic video game from the 90s, Doom. The VizDoom research tool was chosen due to its well-documented API, and more importantly, an environment which only provides *visual feedback* which rules out handicaps for implementers of learning algorithms.\n",
    "\n",
    "Two types of agent interact with the environment; a simple reflex agent operating on the Finite State Machine Model, and an intelligenct agent built with reinforcement learning algorithms Proximal Policy Optimization (PPO) _or_ Advantage Actor-Critic (A2C).\n",
    "\n",
    "Kane and Abel will be trained and evaluated on a single scenario, **Deadly Corridor**.\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\underline {\\large 3.2 Constructing Kane - The building blocks of a Finite State Machine.}\n",
    "\n",
    "\\par\n",
    "\n",
    "During the lifetime of an **episode**, the agent transitions _continuously_ between two phases; movement\n",
    "(moving towards the goal) and action (shooting at detected enemies).\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.8]{images/kane_phases.png}\n",
    "    \\par\n",
    "    \\centering {Figure 3.3: Kane's phases.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "Finite state machines are an abstract machine that can be in exactly one of a finite number of states at any given time (source: Wikipedia). A Finite State Machine(FSM) is also a control system design methodology that describes the behavior or the working principle of the system by transitioning between movement and action\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.8]{images/FSM_kane.png}\n",
    "    \\par\n",
    "    \\centering {Figure 3.4: FSM state diagram for Kane.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "Actions can be simple, or could involve a complex set of processes. Events are triggered by detections by an **object detection** model from visual data. The FSM system transitions from one phase to another based on events. \n",
    "\n",
    "\\underline {Object detection}\n",
    "\n",
    "\\par\n",
    "\n",
    "Object detection is a common task and at the time of writing, machine learning practitioners are spoilt for choice when it comes to choice of frameworks. The following machine learning architectures were available for accelerating the development of Kane's object detection model.\n",
    "\n",
    "1. [Detectron2](https://github.com/facebookresearch/detectron2)\n",
    "2. [YOLOv5](https://github.com/ultralytics/yolov5)\n",
    "3. [Cascade Classifier](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.6]{images/yolov5_test.png}\n",
    "        \\par\n",
    "        \\centering {Figure 3.5: Object detection with YOLOv5.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\\underline {\\large 3.3 Constructing Abel - The Building Blocks of Deep Reinforcement Learning}\n",
    "\n",
    "\\underline {Markov Decision Process}\n",
    "\n",
    "\\normalsize Markov Decision Process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. A Markov Decision Process is a 4-tuple of $S$ state space, $A$ action space, $P_a (s, s')$ is the probability that action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $t+1$, **and**, $R_a (s, s')$ reward is the reward from transition from $s$ to $s'$.\n",
    "\n",
    "In reinforcement learning, MDP is used wherever probabilities or rewards are unknown (Shoham et. al, 2003). Reinforcement learning can solve Markov-Decision processes without explicit specification of the transition probabilities; the values of the transition probabilities are needed in value and policy iteration.\n",
    "\n",
    "\\underline {Convolutional Neural Networks}\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a type of artificial neural network which is used to analyze visual data. CNNs are regularized versions of multilayer perceptrons, typically consist of a convolutional layer, a pooling layer, and a connected layer. CNNs are used to process the visual data from the environment to get a response from the agent. \n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.7]{images/cnn_rl.png}\n",
    "        \\par\n",
    "        Figure 3.6: A simplified representation of CNNs in Reinforcement Learning.\\par\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "\\underline {Deep Reinforcement Learning}\n",
    "\n",
    "In reinforcement learning, instead of explicit specification of the transition probabilities, the transition probabilities are accessed through a simulator that is typically restarted many times from a uniformly random initial state. Reinforcement learning can also be combined with function approximation to address problems with a very large number of states.\n",
    "\n",
    "**Reinforcement learning** is a **generic framework** for representing and solving control tasks. There are a number of algorithms within the framework, and below are some of them:\n",
    "\n",
    "* Associative reinforcement learning\n",
    "* Deep reinforcement learning\n",
    "* Fuzzy reinforcement learning\n",
    "* Inverse reinforcement learning\n",
    "* Safe reinforcement learning\n",
    "* Partially supervised reinforcement learning\n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.6]{images/RL_process.jpg}\n",
    "        \\par\n",
    "        Figure 3.7: Reinforcement learning process. \\par\n",
    "\\end{figure}\n",
    "\n",
    "Implementers are free to choose which algorithms to apply to a particular control task.  Deep learning algorithms are a natural choice as they are able to process complex data efficiently. An RL agent learns by trial and error\n",
    "a good policy (or controller) based on observations and numeric reward feedback on the previously performed action (Buffet et. al., (2020)). In RL, the agent needs to learn how to choose good actions based on its observations and the reward feedback, without necessarily knowing the dynamics of the environment (Buffet et. al., 2020).\n",
    "\n",
    "\\underline {\\large Techniques}\n",
    "\n",
    "\\underline {Reward Shaping}\n",
    "\n",
    "Reward shaping is a method for engineering a reward function in order to provide more frequent feedback on appropriate behaviors (Wiewora, 2017). Reward shaping does not affect the performance of Kane, but is crucial during Abel's training phase. The ViZDoom engine provides a large number of environment variables for use in reward shaping, and the following list of variables are used for Abel's training:\n",
    "\n",
    "* HEALTH - how much health the agent has at the end of an episode, \n",
    "* KILLCOUNT - how many kills did the agent make during an episode, \n",
    "* SELECTED_WEAPON_AMMO - remaining ammo at the end of an episode, \n",
    "* HITS_TAKEN - how many hits the agent took from enemies, \n",
    "* HITCOUNT - how often the agent was able to hit, but not necessarily kill, enemies.\n",
    "\n",
    "\\underline {Curriculum Learning}\n",
    "\n",
    "When the **curriculum learning** technique is described within the context of Abel's training phase, the agent solves a complex target task by reusing action-values across a tailored sequence of related tasks of increasing difficulty. The model starts training on the easiest version of the environment (i.e. enemies which are less deadly) training progresses into progressively more difficult environments.\n",
    "\n",
    "\\underline {Automated hyperparameter tuning}\n",
    "\n",
    "Automated hyperparameter tuning simply automates the tedious task of searching for optimal settings for a task. This technique's efficacy depends on the size of the search space and available processing power to search the entire space. The search stops when there's significant improvement in results over baseline values which were established at the start of the study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8603cb5",
   "metadata": {},
   "source": [
    "\\underline {\\large 3.4 Libraries, Frameworks and Tooling}\n",
    "\n",
    "\\underline {\\large Video game engine}\n",
    "\n",
    "\\normalsize [VizDoom](https://github.com/mwydmuch/ViZDoom) is a software platform for research into autonomous control based on raw visual information. The environment is based on the classic first-person shooter from the 90s, Doom. The environment allows the development of game-playing bots which interpret information based only on the screen buffer.  VizDoom also provides:\n",
    "\n",
    "* a realistic physics model,\n",
    "* customization of environmental parameters, maps, non-player characters, rewards, goals and actions\n",
    "* visual and audio feedback for perception and interpretation by the game-playing agent.\n",
    "\n",
    "First-person shooter (FPS) games such as Unreal Tournament, Quake III Arena and Counter-Strike have been used for A.I research. The drawback of using these games is the availability of high-level information such as the position of walls and enemies. VizDoom's feedback can be access only as _raw visual information_, requiring the agent to respond from purely visual feedback.\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.7]{images/vizdoom.png}\n",
    "    \\par\n",
    "    \\centering {Figure 3.8: ViZDoom game engine.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\\underline {\\large Environment}\n",
    "\n",
    "\\normalsize With a game engine, a 2D environment can be created and simulated. This environment can then be used to synthesize data for the agent to 'consume'.\n",
    "\n",
    "The game engine **environmental properties** would be represented as numerical data, which is used to produce rewards, navigate and shoot.\n",
    "\n",
    "**OpenAI's Gym** wrappers are a convenient way to modify an existing environment without having to alter the underlying code directly. With OpenAI's Gym wrapper, the code can be modular and the same wrapper and environment can be utilised by both Kane and Abel.\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.5]{images/AE_loop_dark.png}\n",
    "    \\par\n",
    "    \\centering {Figure 3.9 OpenAI Gym Loop.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "Gym implements the classic “agent-environment loop”, action spaces and rewards are generated from the game engine's API.\n",
    "\n",
    "\\underline {\\large PyTorch and Stable Baselines3}\n",
    "\n",
    "The machine learning ecosystem is dominated by Tensorflow and Pytorch. Both frameworks work well for most machine learning tasks. PyTorch will be used to implement the reinforcement learning agent because of tooling support, plentiful available code examples, and extensive documentation.\n",
    "\n",
    "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms build on top of PyTorch. Training agents in Stable-Baselines3 takes just a few lines of code, after which the agent can be queried for actions. This allows researchers to easily use the baseline algorithms and components in experiments (e.g. Klink et al. (2020); Nair et al. (2019)). SB3 comes with extensive documentation of the code API and examples on Google's Colab environment.\n",
    "\n",
    "\\underline {\\large Plan of Work}\n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics{images/plan_of_work.png}\n",
    "        \\par\n",
    "        Figure 3.10: Plan of work. \\par\n",
    "\\end{figure}\n",
    "\n",
    "\\underline {\\large Testing and Evaluation}\n",
    "\n",
    "The performance of both agents will be tested on the ViZDoom scenario 'Deadly Corridor'. Deadly Corridor is a representative of the challenges players face in a first-person shooter; collectin power-ups and health items, killing enemies and moving towards a goal. Both agents must navigate a narrow corridor with enemies to reach the end without being killed.\n",
    "\n",
    "\\underline {Testing Kane}\n",
    "\n",
    "\\normalsize Testing Kane in the ViZDoom environment takes a straightforward approach. **Will the FSM's algorithm enable the bot to kill all enemies and reach the end of the corridor to complete the level?**\n",
    "\n",
    "\\underline {Testing Abel - Welch-T Test}\n",
    "The performances of Reinforcement Learning algorithms have specific characteristics. They are independent\n",
    "of each other and they are not paired between algorithms. Statistical difference testing offers a principled way to compare the central performances of two algorithms (Colas et. al., 2019).\n",
    "\n",
    "Testing the performance of the PPO vs. A2C will be done with the **Welch-T test**. This test calculates the T-test for the means of two independent samples of scores. This is a test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default. After running the Welch-T test, a mean will be calculated from 100 episodes for both algorithms.\n",
    "\n",
    " \n",
    "\\pagebreak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73d30f",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\huge 4. Implementation\n",
    "\\end{center}\n",
    "\\par\n",
    "\n",
    "\\underline{\\large 4.1 Environment.}\n",
    "\n",
    "The ViZDoom game engine is 'wrapped' with OpenAI's Gym library, which produces feedback in the form of rewards, action spaces and episodic levels. The environment's visual buffer will be used to train Abel and provide Kane with visual feedback. Each frame of the buffer will be 320 x 240 pixels, and each frame is converted to grayscale. The scenario used for both Kane and Abel is 'Deadly Corridor', a long hallway with enemies and an armour power-up at the end of the level. The goal of both Kane and Abel is to reach the end without being killed by the enemies.\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.4]{images/deadly_corridor.png}\n",
    "    \\par\n",
    "    \\centering {Figure 4.1: Deadly Corridor level.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "The image illustrating the level above shows the following; a **green dot** representing the **agent**, **red dots** representing **enemies** and a single **orange dot** on the far right which is an **armour power-up** and the end of the level.\n",
    "\n",
    "\\underline{\\large 4.2 Implementing Kane}\n",
    "\n",
    "\\underline{\\large Navigation}\n",
    "\n",
    "The dynamic programming technique **A\\* Search** is an improvement of Dijkstra's algorithm, which is a greedy algorithm (Rachmawati et. al., 2019). Dijkstra and A* perform equally well for solving small scale pathfinding (.e.g a town or regional scale maps), but A* performs better with large scale pathfinding.\n",
    "\n",
    "A* search would be run *exactly once* during each episode to solve the map, and the calculated path saved. The agent would move along the calculated path and fire on any enemies it detects.\n",
    "\n",
    "\\underline{\\large Object Detection}\n",
    "\n",
    "\\normalsize Object detection is crucial for implementing Kane's algorithm. The object detection model had to be able to predict the presence of enemies from the game engine's screen buffer. The object detection model was implemented in two stages; \n",
    "\n",
    "1. data collection and annotation,\n",
    "2. object detection model training\n",
    "\n",
    "\\normalsize The training data was annotated manually with LabelImg, which is a GUI image annotation tool. The code for this tool, can be found [here](https://github.com/heartexlabs/labelImg).\n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.6]{images/labelImg.png}\n",
    "        \\par\n",
    "        Figure 4.2: LabelImg in action.\\par\n",
    "\\end{figure}\n",
    "\n",
    "Implementation of the object detection model was attempted with 3 different machine learning architectures, which  were Haar Cascades, Detectron 2 and YOLOv5. YOLOv5 was selected after a few failed attempts with the other architectures.\n",
    "\n",
    "\\underline{\\large Logic for Movement and Action Phases.}\n",
    "\n",
    "Kane starts an episode in the movement phase and moves towards its goal, until enemies are detected. If enemies are detected, the agent transitions to the action phase, and shoots at the enemy. If no enemies are detected, Kane continues moving towards the goal.\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "Initial input (ViZDoomGym Env E, Actions A, Screen Buffer S, Object Detection Model M)\n",
    "\"\"\"\n",
    "\n",
    "environment = E\n",
    "screen_buffer = S\n",
    "actions = A\n",
    "\n",
    "while E.episode is True:\n",
    "    enemies = M(S)\n",
    "    if enemies.length is more than 0 do:\n",
    "      A.turn\n",
    "      if enemies.center equals screen_buffer.center do\n",
    "        A.shoot\n",
    "    else\n",
    "      A.move\n",
    "return E.episode.score\n",
    "```\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\underline{\\large 4.3 Implementing Abel}\n",
    "\n",
    "\n",
    "\\underline {\\large Training models with the PPO and A2C algorithms implemented with the Stable Baselines3 library.}\n",
    "\n",
    "Description of each hyperparameter:\n",
    "\n",
    "* Gamma - quantifies how much importance is given to future rewards.\n",
    "* NStep - the number of steps to run for each environment per update.\n",
    "* Learning Rate - this value controls the rate or speed at which the model learns.\n",
    "* Entropy Coefficient - inverse of the reward scale used for regularization to improve the model's policy optimization\n",
    "* Clip Range - clipping is used to stay in a valid interval when action range is not bounded.\n",
    "* GAE Lambda - a smooting parameter for reducing the variance in training, which produces stabler results.\n",
    "\n",
    "The baseline performance for PPO and A2C is established by training the model for 500,000 time-steps with the baseline hyperparameter values from _Table 4.1_ and _Table 4.2_. \n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Hyperparameter & Value \\\\ [0.5ex] \n",
    " \\hline\\hline\n",
    " Gamma & 0.9 \\\\ \n",
    " \\hline\n",
    " NStep & 256 \\\\\n",
    " \\hline\n",
    " Clip Range & 0.1 \\\\\n",
    " \\hline\n",
    " Learning Rate & 0.0001 \\\\\n",
    " \\hline\n",
    " GAE Lambda & 0.9 \\\\\n",
    " \\hline\n",
    " Network Size & (32, medium) \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 4.1: Default hyperparameter values for PPO.\n",
    "\\end{center}\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Hyperparameter & Value \\\\ [0.5ex] \n",
    " \\hline\\hline\n",
    " Gamma & 0.90 \\\\ \n",
    " \\hline\n",
    " NStep & 128 \\\\\n",
    " \\hline\n",
    " Learning Rate & 0.0001 \\\\\n",
    " \\hline\n",
    " GAE Lambda & 0.90 \\\\\n",
    " \\hline\n",
    " Network Size & (32, medium) \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 4.2: Default hyperparameter values for A2C.\n",
    "\\end{center}\n",
    "\n",
    "\n",
    "\n",
    "\\underline {Phase 1 - Hyperparameter tuning with Optuna}\n",
    "\n",
    "Both algorithms ran for 100,000 timesteps for 100 trials. The optimal hyperparameters are recorded in a SQLite database. At the end of 100 trials, the hyperparameters which produced the best score is used for Stage Two.\n",
    "\n",
    "The automation is done with Optuna, a library for implementing automated hyperparameter tuning for machine learning models. Optuna boasts an efficient sampling and pruning mechanism, easy scalability and ease of setup (Takuya et. al. 2019). \n",
    "\n",
    "The optimized hyperparameter after 100 trials for both PPO and A2C are listed in _Table 4.3_ and _Table 4.4_ respectively.\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Hyperparameter & Value \\\\ [0.5ex] \n",
    " \\hline\\hline\n",
    " Gamma & 0.99 \\\\ \n",
    " \\hline\n",
    " NStep & 8192 \\\\\n",
    " \\hline\n",
    " Entropy Coefficient & 0.005 \\\\\n",
    " \\hline\n",
    " Clip Range & 0.1 \\\\\n",
    " \\hline\n",
    " Learning Rate & 0.00001 \\\\\n",
    " \\hline\n",
    " GAE Lambda & 0.9 \\\\\n",
    " \\hline\n",
    " Network Size & (64, medium) \\\\\n",
    " \\hline\n",
    " Activation Function & tanh \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 4.3: Optimized hyperparameter values for PPO.\n",
    "\\end{center}\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Hyperparameter & Value \\\\ [0.5ex] \n",
    " \\hline\\hline\n",
    " Gamma & 0.99 \\\\ \n",
    " \\hline\n",
    " NStep & 128 \\\\\n",
    " \\hline\n",
    " Entropy Coefficient & 0.0007 \\\\\n",
    " \\hline\n",
    " Learning Rate & 0.003 \\\\\n",
    " \\hline\n",
    " GAE Lambda & 0.99 \\\\\n",
    " \\hline\n",
    " Network Size & (32, small) \\\\\n",
    " \\hline\n",
    " Activation Function & tanh \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 4.4: Optimized hyperparameter values for A2C.\n",
    "\\end{center}\n",
    "\n",
    "\\underline {Phase 2 - Training}\n",
    "\n",
    "Curriculum training is introduced at this phase and divided into **5 difficulty levels**.\n",
    "PPO and A2C are trained for time-steps with optimized hyperparameters. Versions of the model are saved at intervals of 10,000 time-steps. The model at the timestep with the best `mean reward` is used for training the model at the next difficulty level.\n",
    "\n",
    "\\begin{figure}\n",
    "        \\centering\n",
    "        \\includegraphics[scale=0.6]{images/best_model.png}\n",
    "        \\par\n",
    "        Figure 4.3: The saved model at timestep 400,000 (circled blue) is used for the next difficulty level.\\par\n",
    "\\end{figure}\n",
    "\n",
    "The training pipeline includes reward shaping and curriculum learning techniques. At the end of Stage 2, the both models are evaluated with the by running each for 10 episodes and calculating the mean.\n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80854ccc",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\huge 5. Evaluation\n",
    "\\end{center}\n",
    "\n",
    "---\n",
    "\n",
    "The results of implementation for Kane and Abel are presented in this section. Kane's algorithms are evaluated on the basis of level completion; did the agent reach the goal after killing all enemies? Abel's implementation between the PPO and A2C algorithms are compared in detail; results of the hyperparameter tuning trials, performance of baseline vs. optimized models and final results.\n",
    "\n",
    "\n",
    "\\underline{\\large 5.1 Evaluating Kane}\n",
    "\\par\n",
    "\\underline{\\large Kane - Pathfinding}\n",
    "\n",
    "\\normalsize With the A* Search algorithm, a path to the goal is be calculated before the agent started a session. Map features are converted to black and white to replicate a maze like environment. The pathfinding algorithm is not run in real-time due to the lengthy calculations. A path is plotted once and saved to a JSON file for the agent(Kane) to reach its goal.\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.7]{images/a_deadly.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.1: Pathfinding in the Deadly Corridor scenario.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.7]{images/a_way.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.2: Pathfinding in the My Way Home scenario.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\underline{\\large Kane - Actions and Scoring}\n",
    "\n",
    "The agent was able to detect **every** enemy in the **Deadly Corridor** map. Each enemy was killed and the agent reached its goal. One factor to note; enemy attacks on the agent were deadly and the agent's success was dependent on the enemies missing their attacks on the agent.\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.6]{images/kane_shot.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.3: Kane shooting detected enemies.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "Kane's performance was a constant; results would be consistent if every environment variable (i.e. enemy behaviour, number of power-ups) is consistent.\n",
    "\n",
    "\\par\n",
    "\n",
    "\\underline{\\large 5.2 Evaluating Abel}\n",
    "\\par\n",
    "\\underline{\\large Abel}\n",
    "\n",
    "Both models (PPO and A2C) underwent curriculum training with tuned hyperparameter settings. Each model's average reward over 500,000 episodes and accompanying P-value(s) is explained in the coming sections. \n",
    "\n",
    "\\underline{Abel - PPO}\n",
    "\n",
    "A very low P-value indicates the trend of rewards values for this agent increased after curriculum learning over 2,500,000 time-steps.\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Hyperparameter & Value \\\\ [0.5ex] \n",
    " \\hline\n",
    " Learning rate & 0.00001 \\\\ \n",
    " \\hline\n",
    " Clip Range & 0.1 \\\\ \n",
    " \\hline\n",
    " Nepochs & 15 \\\\\n",
    " \\hline\n",
    " GAE Lambda & 0.9 \\\\\n",
    " \\hline\n",
    " Network size & (62, medium) \\\\\n",
    " \\hline\n",
    " GAE Lambda & 0.9 \\\\\n",
    " \\hline\n",
    " Activation Function & Tanh \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 5.1: Final hyperparameter values used to train the PPO model.\n",
    "\\end{center}\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.6]{images/ppo_baseline.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.4: Baseline PPO - reward over timesteps.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "The training reward values show instability with baseline hyperparameter values. Optimized values resulted in stable reward gains, see _Figure 5.5_, below.\n",
    " \n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.6]{images/ppo_end.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.5: Optimized PPO - reward over timesteps.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Measurement & Value \\\\ [0.5ex] \n",
    " \\hline\\hline\n",
    " Baseline Mean & 0.2664 \\\\ \n",
    " \\hline\n",
    " Optimized Mean & 0.6275 \\\\\n",
    " \\hline\n",
    " P-value (Welch-T) & $2.2745 e^{-10}$ \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 5.2: Mean of rewards over 500,000 time-steps.\n",
    "\\end{center}\n",
    "\n",
    "\\underline{Abel - A2C}\n",
    "\n",
    "In an **unexpected** turn of events, hyperparameter tuning did not produce a better performing model than the baseline model. Instead of training the A2C model with hyperparameter values produced from the trials, the A2C model was trained with baseline values for the hyperparameters `learning_rate` and `n_steps`. A possible explanation for the poor performance of the model with tuned hyperparameters is the clipped values from the **advantage function**, which meant the agent was not able to **predict the extra reward it gets if it take an action at another state compared to the mean reward at the current state**.\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Hyperparameter & Value \\\\ [0.5ex] \n",
    " \\hline\n",
    " Learning rate & 0.001 \\\\ \n",
    " \\hline\n",
    " Nsteps & 128 \\\\ [1ex]\n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 5.3: Final hyperparameter values used to train the A2C model.\n",
    "\\end{center}\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.6]{images/a2c_baseline.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.6: Baseline A2C - reward over timesteps.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.6]{images/a2c_end.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.7: Optimized A2C - reward over timesteps.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Measurement & Value \\\\ [0.5ex] \n",
    " \\hline\n",
    " Baseline Mean & 0.6125 \\\\ \n",
    " \\hline\n",
    " Optimized Mean & 0.7278 \\\\\n",
    " \\hline\n",
    " P-value (Welch-T) & 0.1439 \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 5.4: Mean of rewards over 500,000 time-steps.\n",
    "\\end{center}\n",
    "\n",
    "\n",
    "\\underline{Abel - Conclusion}\n",
    "\n",
    "Based on the P-values from _Table 5.5_ below for the PPO and A2C models, the PPO model benefited from hyperparameter tuning, reward shaping and curriculum learning. The A2C algorithm did not benefit from hyperparameter tuning, but was able to take slight advantage of reward shaping and curriculum learning.\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Algorithm & P-Value \\\\ [0.5ex] \n",
    " \\hline\n",
    " PPO & $2.2745 e^{-10}$ \\\\ \n",
    " \\hline\n",
    " A2C & 0.1439 \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 5.5: Comparison of P-values between PPO and A2C.\n",
    "\\end{center}\n",
    "\n",
    "**Conclusion**: The PPO algorithm gave better averaged performance than A2C.\n",
    "\\par\n",
    "\n",
    "\\underline{\\large 5.3 Testing and Comparison}\n",
    "\\par\n",
    "\\underline{\\large Getting Mean And Selecting An Overall Winner.}\n",
    "\n",
    "The simplest way to evaluate the results is to plot a reward vs. time-step graph. The algorithm which consistently earns more rewards over time-steps is the clear winner - which is the PPO algorithm, based on the figures in _Table 5.6_. \n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.8]{images/mean_rewards_100_episodes.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.8: Rewards over 100 episodes (green - FSM, yellow - PPO, red - A2C).\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Algorithm & Rewards(mean) \\\\ [0.5ex] \n",
    " \\hline\\hline\n",
    " FSM & 0.1828 \\\\ \n",
    " \\hline\n",
    " PPO & 1.2052 \\\\\n",
    " \\hline\n",
    " A2C & 1.1955 \\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\par\n",
    "Table 5.6: Mean of accumulated rewards for 100 episodes for all 3 algorithms.\n",
    "\\end{center}\n",
    "\n",
    "\\underline{\\large \\textbf{5.4 Conclusion}}\n",
    "\n",
    "\\large \\textbf{The Abel-PPO algorithm gave the best performance.}\n",
    "\n",
    "\\normalsize The PPO model's advantage over A2C was slight, which suggested the PPO algorithm was better at saving ammo and avoiding hits (i.e. the agent killed enemies rapidly to avoid being targeted and shot at. The evidence is in the Frames-Per-Second(FPS) metric, as seen below in _Figure 5.9_ and _Figure 5.10_.\n",
    "\n",
    "Higher FPS values meant the A2C agent was spending more time **exploring the environment**, which meant it was more likely to encounter enemies and **lose health**.\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.6]{images/ppo_fps.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.9: PPO FPS.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[scale=0.6]{images/a2c_fps.png}\n",
    "    \\par\n",
    "    \\centering {Figure 5.10: A2C FPS.\\par}\n",
    "\\end{figure}\n",
    "\n",
    "The implementations of Kane and Abel will be discussed in the coming section, \\textbf{6. Conclusion and Further Work}.\n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f49c",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\huge 6. Conclusion and Further Work\n",
    "\\end{center}\n",
    "\n",
    "---\n",
    "\n",
    "\\large \\textbf{Conclusion - The best algorithm was Abel - Proximal Policy Optimization.}\n",
    "\\par\n",
    "\n",
    "\\underline {Primary Objective}: **What are the strengths and weaknesses of dynamic programming and deep reinforcement learning algorithms in a controlled lab setting?**\n",
    "\n",
    "There were a couple of glaring issues with the implementation of Kane and Abel.\n",
    "\n",
    "Kane's algorithm works well on different maps with some additional work, such as training the Object Detection model on new monsters. Kane is only able to reach a scenario's goals in 'God Mode' due to the agent's inability to quickly turn and target enemies. This is partly due to the ViZDoom environment's turning velocity - turn too fast and the targeting crosshairs overshoot the enemy's bounding box on-screen. Turning slowly works, but the agent would die from deadly enemy fire. **Kane is also unable to independently 'infer' the goals of a scenario as Kane's algorithm does not respond to rewards**, and requires the researcher to manually enter the co-ordinates for Kane to reach.\n",
    "\n",
    "A common theme to improving most Reinforcement Learning is to increase the ability of the trained model to **generalize**. Could trained agents  be \"memorizing\" action sequences in some sense, and exploiting nuances of states to remember which action to take? Abel's model performed poorly on other scenarios, such as 'My Way Home', which required the agent to collect every power-up available on the map. Training a Reinforcement Learning agent took massive amounts of trial and error to correctly tweak hyperparameter settings. **The 'Reward Shaping' technique used for this project contributed** to Abel's inability to generalize; for example, for a scenario which required killing all enemies, the reward for kills had to be higher than in scenarios which required the agent to collect a unique power-up to win.\n",
    "\n",
    "Utilizing Optuna allowed rapid, automated testing of various hyperparameter settings and policy settings. The automated trials with Optuna produced results early; trial number 3 for A2C and trial number 4 for PPO out of a total of 100 trials. The suggested hyperparameter values were mostly kept during training, except for the **learning rate**, which was reduced to `1e5`. Learning rates which are too high are unstable and rates which are too small results in a model which fails to learn.\n",
    "\n",
    "\\underline {Secondary Objective}: **Between Proximal Policy Optimization and Actor 2 Critic, which deep reinforcement learning algorithm would be easier to tweak and train?**\n",
    "\n",
    "Proximal Policy Optimization was the easier algorithm to apply curriculum learning and reward shaping with. Both algorithms showed improvements as evidenced by the T-scores. With the A2C algorithm, the actor(the A in A2C) is trained by producing an advantage (from the \"next state\" as input and the critic) and using that to change the probability of producing that action given that state. PPO removes most of the affect of the advantage with clipping, and the actor's action distribution for a particular state doesn't diverge much during training.\n",
    "\n",
    "In other words, A2C is a framework while PPO is a policy optimization method.\n",
    "\n",
    "\\underline {Secondary Objective}: **Are reward shaping, automated hyperparameter tuning, and curriculum learning good strategies for training deep reinforcement learning-based IAs?**\n",
    "\n",
    "Automated hyperparameter tuning was a good strategy. Tuning with Optuna eliminated countless manual trials without jeopardizing deadlines. The optimized models were able to learn tasks well, however, not all optimized hyperparameters were created equally. i.e. The learning rate had to be tweaked to eliminate unstable training loss functions.\n",
    "\n",
    "Reward values which work well for PPO have the opposite effect on A2C. This is probably true for any algorithm - rewards optimized for a policy will not work for another policy. Just like reward shaping, curriculum learning have **divergent degrees** of influence on training efficiency. Increasing difficulty on a model which has not 'learned' certain techniques to succeed in an environment will have **catastrophic effects**. i.e. If an agent was able to rush to an exit to win a level at lower difficulties without killing enemies, it would not be able to do so at higher difficulties as enemies get tougher.\n",
    "\n",
    "\\underline {\\large Further Work}\n",
    "\n",
    "\\underline {Additional Test Cases}\n",
    "\n",
    "The ability of the model to perform in environments which it has not 'seen' can be tested with new maps with the same enemies. When concerned with only a single environment, standard concentration inequalities can compute\n",
    "confidence intervals on the mean performance. Similarly, when displaying the distribution of performance across multiple environments,  standard techniques for bounding the empirical distribution of performance can be applied (Jordan et. al. 2020).\n",
    "\n",
    "\\underline {Applying Discretization Methods to Data}\n",
    "\n",
    "Neural networks can forget what they learned in the past due to a phenomenon known as **catastrophic interference**. Interference happens when a neural network is trained on new data and it overwrites what it has learned in the past (Ghiassian et. al., 2020).\n",
    "\n",
    "A couple of approaches are suggested by Ghiassian et. al., 2020. First is to discretize each dimension of the input separately (e.g. images divided into Red, Green and Blue layers) and perform one-hot encoding on each layer. Second is to use **Tile Coding** (TC) (Albus 1975, 1981). After breaking each image into tiles, **Tile Coding** creates a representation for each point in space by concatenating the representation it has for each tiling. While discretization methods are highly scalable, but it remains unclear how effective it will be in higher dimensional spaces.\n",
    "\n",
    "\\underline {Larger Neural Networks}\n",
    "\n",
    "The neural networks architecture for the PPO and A2C algorithms had two shared layers with 64 hidden layers each. In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favour smaller models (Novak et., al., 2018). \n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1577d6",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\huge Appendix A: Bibliography\n",
    "\\end{center}\n",
    "\n",
    "---\n",
    "\n",
    "A F Pukeng1, R R Fauzi., Lilyana, R Andrea, E Yulsilviana, S Mallala (2017) \"An intelligent agent of finite state machine in educational game “Flora the Explorer”\n",
    "Link: [https://iopscience.iop.org/article/10.1088/1742-6596/1341/4/042006/pdf](https://iopscience.iop.org/article/10.1088/1742-6596/1341/4/042006/pdf)\n",
    "\n",
    "Adegun, A.A., Ogundoken, O. R., Ogbonyomi S., Sadiku, P. O., \"Design and Implementation of an Intelligent Gaming Agent Using A* Algorithm and Finite State Machines\"\n",
    "Link: [https://www.researchgate.net/publication/346319155_Design_and_Implementation_of_an_Intelligent_Gaming_Agent_Using_A_Algorithm_and_Finite_State_Machines](https://www.researchgate.net/publication/346319155_Design_and_Implementation_of_an_Intelligent_Gaming_Agent_Using_A_Algorithm_and_Finite_State_Machines)\n",
    "\n",
    "Asada, M., Shoichi, N., Tawaratsumida, S., Hosoda, K., \"Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning\"\n",
    "Link: [https://link.springer.com/book/10.1007/978-1-4613-0471-5](https://link.springer.com/book/10.1007/978-1-4613-0471-5)\n",
    "\n",
    "Ashfahani, A., Pratama, M., (2022), \"Autonomous Deep Learning: Continual Learning Approach For Dynamic Environments.\"\n",
    "Link: [https://arxiv.org/pdf/1810.07348.pdf](https://arxiv.org/pdf/1810.07348.pdf)\n",
    "\n",
    "Buffet, O., Pietquin, O., Weng, P., (2020), \"Reinforcement Learning\"\n",
    "Link: [https://arxiv.org/pdf/2005.14419.pdf](https://arxiv.org/pdf/2005.14419.pdf)\n",
    "\n",
    "Cestero, J., Quartulli, M., Metelli, M.M., Restelli, M., (2022) \"Storehouse: a Reinforcement Learning Environment\n",
    "for Optimizing Warehouse Management\"\n",
    "Link: [https://arxiv.org/pdf/2207.03851.pdf](https://arxiv.org/pdf/2207.03851.pdf)\n",
    "\n",
    "Colas, C., Sigaud, O., Oudeyer, P., (2019) \"A Hitchhiker’s Guide to Statistical Comparisons of Reinforcement Learning Algorithms\"\n",
    "Link: [https://openreview.net/pdf?id=ryx0N3IaIV](https://openreview.net/pdf?id=ryx0N3IaIV)\n",
    "\n",
    "Daniel, K., Nash, A., and Koenig, S. 2010. \"Theta*: Any-angle path planning on grids.\" Journal of Artificial Intelligence Research, 39, 533–579.\n",
    "\n",
    "Fakoor, R., Chaudhari, P., Smola, A. J., (2019) \"P3O: Policy-on Policy-off Policy Optimization\"\n",
    "Link: [https://arxiv.org/pdf/1905.01756.pdf](https://arxiv.org/pdf/1905.01756.pdf)\n",
    "\n",
    "Ghiassian, S., Rafiee, B., Lo, Y., White, A., (2020) \"Improving Performance in Reinforcement Learning by\n",
    "Breaking Generalization in Neural Networks\"\n",
    "Link: [https://arxiv.org/pdf/2003.07417.pdf](https://arxiv.org/pdf/2003.07417.pdf)\n",
    "\n",
    "Hamalainen, Perttu, Babadi, Amin, Ma, Xiaoxiao, and Lehtinen, Jaakko. PPO-CMA: proximal policy optimization with covariance matrix adaptation.\n",
    "\n",
    "Hart, P. E.; Nilsson, N. J.; Raphael, B. (1968). \"A Formal Basis for the Heuristic Determination of Minimum Cost Paths\". *IEEE Transactions on Systems Science and Cybernetics*. **4** (2): 100–107. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1109/TSSC.1968.300136](https://doi.org/10.1109%2FTSSC.1968.300136).\n",
    "\n",
    "Iversen, A. & Taylor, Nick & Brown, Keith. (2005). Classification and verification through the combination of the multi-layer perceptron and auto-association neural networks. Proceedings of the International Joint Conference on Neural Networks. 2. 1166 - 1171 vol. 2. 10.1109/IJCNN.2005.1556018. \n",
    "\n",
    "Jordan, S., Chandak Y., Cohen D., Zhang, M., Thomas, Philip., (2020) \"Evaluating the Performance of Reinforcement Learning Algorithms\"\n",
    "Link: [https://arxiv.org/pdf/2006.16958.pdf](https://arxiv.org/pdf/2006.16958.pdf)\n",
    "\n",
    "\n",
    "Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida, and Koh Hosoda. \"Purposive behavior acquisition for a real robot by vision-based reinforcement learning\". In Recent Advances in Robot Learning, pages 163–187.\n",
    "\n",
    "Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor E., M., Stone, P., (2020), \"Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey\"\n",
    "Link: [https://arxiv.org/pdf/2003.04960.pdf](https://arxiv.org/pdf/2003.04960.pdf)\n",
    "\n",
    "Novak, R., Bahri, Y., Abolafia, D., Pennington, J., Sohl-Dickstein, J., (2018 ) \"SENSITIVITY AND GENERALIZATION\n",
    "IN NEURAL NETWORKS: AN EMPIRICAL STUDY\"\n",
    "Link: [https://arxiv.org/pdf/1802.08760.pdf](https://arxiv.org/pdf/1802.08760.pdf)\n",
    "\n",
    "O'Shea, K., Nash, R., \"An Introduction to Convolutional Neural Networks\", (2015).\n",
    "Link: [https://arxiv.org/pdf/1511.08458.pdf](https://arxiv.org/pdf/1511.08458.pdf)\n",
    "\n",
    "Pascal Klink, Hany Abdulsamad, Boris Belousov, and Jan Peters. \"Self-paced contextual reinforcement learning\". In Conference on Robot Learning, pages 513–529, 2020.\n",
    "\n",
    "Schulze, C., Schulze, M., 2018. \"ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling*\"\n",
    "Link: [https://arxiv.org/pdf/1801.01000.pdf](https://arxiv.org/pdf/1801.01000.pdf)\n",
    "\n",
    "Shoham, Y.; Powers, R.; Grenager, T. (2003). \"Multi-agent reinforcement learning: a critical survey\" (PDF). Technical Report, Stanford University: 1–13. Retrieved 2018-12-12.\n",
    "\n",
    "Simonini, T., Sanseviero, O., (2019) \"An Introduction to Deep Reinforcement Learning\"\n",
    "Link: [https://huggingface.co/blog/deep-rl-intro](https://huggingface.co/blog/deep-rl-intro)\n",
    "\n",
    "Suraj Nair, Yuke Zhu, Silvio Savarese, and Fei-Fei Li. 2019 \"Causal induction from visual observations for goal directed tasks.\"\n",
    "Link: [https://arxiv.org/pdf/1910.01751.pdf](https://arxiv.org/pdf/1910.01751.pdf)\n",
    "\n",
    "Takuya A., Shotaro S., Toshihiko Y., Takeru O., Masanori K., (2019), \"Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD.\"\n",
    "Link: [https://arxiv.org/pdf/1907.10902.pdf](https://arxiv.org/pdf/1907.10902.pdf)\n",
    "\n",
    "Wiewora E., (2017), \"Reward Shaping\", Encyclopedia of Machine Learning. pp 863 - 865.\n",
    "Link: [https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_731](https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_731)\n",
    "\n",
    "Zhao, K., Tang Z., Yuanheng Z., Nannan L., Zhao D., (2019) \"A Survey of Deep Reinforcement Learning in Video\n",
    "Games\"\n",
    "Link: [https://arxiv.org/pdf/1912.10944.pdf](https://arxiv.org/pdf/1912.10944.pdf)\n",
    "\n",
    "Zhao, Z., Zheng, P., Xu, S., Wu, X. (2019) \"Object Detection with Deep Learning: A Review\"\n",
    "Link: [https://arxiv.org/pdf/1807.05511.pdf](https://arxiv.org/pdf/1807.05511.pdf)\n",
    "\n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61853338",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\huge Appendix B: User Guide\n",
    "\\end{center}\n",
    "\n",
    "---\n",
    "\n",
    "\\underline {\\large Environment Setup}\n",
    "\n",
    "This section describes running the project in a Unix based environment.\n",
    "\n",
    "1. clone this project from Github by doing `git clone https://github.com/saintograph/cm3070_final_project`\n",
    "2. install Python and PyTorch. The version of Python used in the development of this project is Python `3.8.10`. Instructions for installing PyTorch can be found here: https://pytorch.org/get-started/locally/\n",
    "3. install all packages with `python -m pip install -r requirements.txt`. This installs all of the packages required to run this project.\n",
    "\n",
    "\\underline {\\large Running demos}\n",
    "\n",
    "This section describes how to run the demos with saved weights.\n",
    "\n",
    "1. download the saved weights from [https://www.dropbox.com/sh/l1lmlyxjuxb4glh/AAAvHKuZhmSj2NJfeXaEW37Da?dl=0](https://www.dropbox.com/sh/l1lmlyxjuxb4glh/AAAvHKuZhmSj2NJfeXaEW37Da?dl=0) and copy the files to the 'weights' directory. Do no _unzip_ the files.\n",
    "2. run the PPO agent demonstration with `python demo_ppo.py`.\n",
    "3. run the A2C agent demonstration with `python demo_a2c.py`.\n",
    "4. run the FSM agent with `python fsm.py`.\n",
    "\n",
    "\\underline {\\large Running Optuna trials}\n",
    "\n",
    "Optuna trials were used to find optimal hyperparameter settings within a search space. To view results, a tool such as https://sqlitebrowser.org/ could be used to browse the hyperparameter values stored in `trials_{ALGORITHM}.db` after a successful trial. There are two existing `.db` files submitted, `trials_A2C.db` and `trials_PPO.db`.\n",
    "\n",
    "1. run trials with `python trials.py PPO 10000`. The preceeding command runs 100 trials for the PPO algorithm for 10000 time-steps each trial. Note the upper-case characters when specifying an algorithm.\n",
    "2. for A2C, run `python trials.py A2C 10000`.\n",
    "3. a SQLite database is created in the root folder when a trial is initiated.\n",
    "4. the search space for PPO and A2C can be defined in the `params.py` file.\n",
    "5. at the end of 100 trials, the trial which produced the best reward values will be displayed on the command line and accessible by browsing the `trial_values` table in the generated `.db` file.\n",
    "\n",
    "\n",
    "\\underline {\\large Training Agents}\n",
    "\n",
    "After acquiring optimal hyperparameter settings, it's time to train the agents! Each model is trained 500,000 times at 5 difficulty levels (i.e. total = 5 * 500000).\n",
    "\n",
    "1. to train the PPO model, use `python main_ppo.py`\n",
    "2. to train the A2C model, use `python main_a2c.py`\n",
    "3. logs are saved to the `logs` folder and training weights to the `train` folder.\n",
    "\n",
    "\\underline {\\large Evaluation Kane and Abel}\n",
    "\n",
    "Kane and Abel are evaluated with simple measurements - the agent which performs best over 100 episodes is the winner!\n",
    "\n",
    "\n",
    "1. run `python evaluate.py`. The script loads weights from the training phase and generates a CSV file.\n",
    "2. to generate the P-value and mean for the training phase, run `python stats.py` to generate results for both PPO and A2C.\n",
    "\n",
    "\\underline {\\large Notes}\n",
    "\n",
    "The script for training YOLOv5 was not included with the project, but some sample training and validation data can be found in the `sample_yolo_training_images` folder. All of the training data was annotated with [LabelImg](https://github.com/heartexlabs/labelImg).\n",
    "\n",
    "To train the a YOLOv5 object detection model, instructions can be found here: https://github.com/ultralytics/yolov5\n",
    "\n",
    "1. run `git clone https://github.com/ultralytics/yolov5` \n",
    "2. copy the `doom.yaml` file from `sample_yolo_training_images` to the root folder of the cloned repo\n",
    "3. create a `demon` folder, and copy the contents of the `train` and `val` folder to it.\n",
    "4. run `python train.py --batch 64 --epochs 100 --data doom.yaml --weights yolov5s.pt` to generate weights\n",
    "\n",
    "The Path Planning implementation of A* search was reused from https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathPlanning/AStar/a_star.py with small modifications.\n",
    "\n",
    "Simply run the script with:\n",
    "\n",
    "1. `cd PathPlanning`\n",
    "2. `python a_star.py`.\n",
    "\n",
    "\\pagebreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
